{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad791c33-4e88-41c6-a432-e42d54a92285",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BEACON median three-panel analysis (GP, length scale, and FP distance).\n",
    "\n",
    "Applies to all molecular systems. Only the fingerprint minima references\n",
    "(minima_fps.csv) need to be updated per molecule to match its minima\n",
    "fingerprints. All other components—data parsing, GP processing, length-scale\n",
    "extraction, and plotting—remain identical for reproducibility.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "# Configuration\n",
    "PRIOR_DIR = Path.cwd()\n",
    "LOGNAME = \"beacon_run.log\"\n",
    "INCLUDE_INIT_MIN = True\n",
    "\n",
    "# Run discovery and log parsing\n",
    "def immediate_run_dirs(base: Path):\n",
    "    dirs = []\n",
    "    for p in sorted(base.iterdir()):\n",
    "        if p.is_dir() and (p / LOGNAME).exists():\n",
    "            dirs.append(p)\n",
    "    if not dirs:\n",
    "        for pat in (\"run_*\", \"seed_*\"):\n",
    "            dirs = [p for p in sorted(base.glob(pat)) if (p / LOGNAME).exists()]\n",
    "            if dirs:\n",
    "                break\n",
    "    return dirs\n",
    "\n",
    "run_dirs = immediate_run_dirs(PRIOR_DIR)\n",
    "if not run_dirs:\n",
    "    kids = [p.name for p in sorted(PRIOR_DIR.iterdir())]\n",
    "    raise RuntimeError(\n",
    "        f\"No runs detected under: {PRIOR_DIR}\\n\"\n",
    "        f\"Expected subfolders with '{LOGNAME}'.\\n\"\n",
    "        f\"Found: {kids}\"\n",
    "    )\n",
    "print(f\"[FOUND] {len(run_dirs)} run folder(s): {[d.name for d in run_dirs][:6]}{' ...' if len(run_dirs)>6 else ''}\")\n",
    "\n",
    "rx_hdr  = re.compile(r\"\\[INFO\\]\\s+Run-ID:\\s+(?P<rid>\\d+).+RNG seed:\\s+(?P<seed>\\d+)\")\n",
    "rx_init = re.compile(r\"Init structure\\s+\\d+\\s+Energy:\\s+([+-]?\\d+(?:\\.\\d+)?(?:[eE][+-]?\\d+)?)\")\n",
    "rx_best = re.compile(r\"\\[BEST\\]\\s+Best-so-far energy:\\s+([+-]?\\d+(?:\\.\\d+)?(?:[eE][+-]?\\d+)?)\")\n",
    "\n",
    "def parse_energy_log(log_path: Path):\n",
    "    run_id = seed = None\n",
    "    init_E, best_series, it = [], [], 0\n",
    "    for line in log_path.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines():\n",
    "        m = rx_hdr.search(line)\n",
    "        if m:\n",
    "            run_id = int(m.group(\"rid\")); seed = int(m.group(\"seed\"))\n",
    "        m = rx_init.search(line)\n",
    "        if m:\n",
    "            init_E.append(float(m.group(1)))\n",
    "        m = rx_best.search(line)\n",
    "        if m:\n",
    "            it += 1\n",
    "            best_series.append((it, float(m.group(1))))\n",
    "    if not best_series and not init_E:\n",
    "        return None\n",
    "    if INCLUDE_INIT_MIN and init_E:\n",
    "        best_series = [(0, float(np.min(init_E)))] + best_series\n",
    "    return dict(run=run_id, seed=seed, series=best_series)\n",
    "\n",
    "runs_meta = []\n",
    "for d in run_dirs:\n",
    "    rec = parse_energy_log(d / LOGNAME)\n",
    "    if rec:\n",
    "        if rec[\"run\"] is None:\n",
    "            suf = d.name.split(\"_\")[-1]\n",
    "            rec[\"run\"] = int(suf) if suf.isdigit() else d.name\n",
    "        runs_meta.append(rec)\n",
    "if not runs_meta:\n",
    "    raise RuntimeError(f\"Logs found but unparsable: ensure '{LOGNAME}' has [INFO]/[BEST] lines.\")\n",
    "\n",
    "rows = []\n",
    "for rec in runs_meta:\n",
    "    for it, e in rec[\"series\"]:\n",
    "        rows.append({\"run\": rec[\"run\"], \"iter\": it, \"best_E\": e})\n",
    "df_long = pd.DataFrame(rows).sort_values([\"run\", \"iter\"])\n",
    "piv = df_long.pivot(index=\"iter\", columns=\"run\", values=\"best_E\").sort_index()\n",
    "\n",
    "global_min_eV = float(np.nanmin(piv.values))\n",
    "delta_mev = (piv - global_min_eV) * 1000.0\n",
    "delta_mev = delta_mev.clip(lower=0)\n",
    "median_mev = delta_mev.median(axis=1)\n",
    "abs_dev = (delta_mev.sub(median_mev, axis=0)).abs()\n",
    "median_run_id = int(abs_dev.sum(axis=0).idxmin())\n",
    "\n",
    "prior_tag = PRIOR_DIR.name\n",
    "parent_name = PRIOR_DIR.parent.name\n",
    "mol = parent_name.replace(\"project_root_\", \"\") if parent_name.startswith(\"project_root_\") else parent_name\n",
    "label = f\"{mol} - {prior_tag} - run {median_run_id}\"\n",
    "\n",
    "run_dir = PRIOR_DIR / f\"run_{median_run_id}\" if (PRIOR_DIR / f\"run_{median_run_id}\").exists() else None\n",
    "if run_dir is None:\n",
    "    candidates = [d for d in run_dirs if d.name.endswith(f\"_{median_run_id}\")]\n",
    "    run_dir = candidates[0] if candidates else run_dirs[0]\n",
    "print(f\"[Energy] median trajectory = {run_dir.name} | global min = {global_min_eV:.6f} eV\")\n",
    "\n",
    "# Clean GP CSV\n",
    "CSV_IN, CSV_OUT = \"gp_prior_vs_step.csv\", \"gp_prior_vs_step.cleaned.csv\"\n",
    "STEP_EPS, VAL_ATOL = 1e-9, 1e-12\n",
    "RENAME = {\"GP\":\"E_gp_pred_eV\",\"GP_var\":\"Var_E_gp_pred_eV2\",\"Prior\":\"E_prior_eV\",\"Step\":\"Step\"}\n",
    "\n",
    "def _load_structured(csv_path: Path):\n",
    "    A = np.genfromtxt(csv_path, delimiter=\",\", names=True, dtype=float, encoding=None)\n",
    "    if A.size == 0:\n",
    "        raise SystemExit(f\"[ERR] empty CSV: {csv_path}\")\n",
    "    if A.ndim == 0:\n",
    "        A = np.array([tuple(A.tolist())], dtype=A.dtype)\n",
    "    return A\n",
    "\n",
    "def _relabel(A):\n",
    "    old = list(A.dtype.names or []); new = [RENAME.get(n, n) for n in old]\n",
    "    if old == new:\n",
    "        return A\n",
    "    B = np.zeros(A.shape, dtype=[(n, float) for n in new])\n",
    "    for o, n in zip(old, new):\n",
    "        B[n] = A[o].astype(float)\n",
    "    return B\n",
    "\n",
    "def _ensure_cols(A):\n",
    "    names = list(A.dtype.names or []); need=[]\n",
    "    if \"Var_E_gp_pred_eV2\" not in names:\n",
    "        need.append((\"Var_E_gp_pred_eV2\", float, np.nan))\n",
    "    if \"E_prior_eV\" not in names:\n",
    "        need.append((\"E_prior_eV\", float, np.nan))\n",
    "    if not need:\n",
    "        return A\n",
    "    B = np.zeros(A.shape, dtype=A.dtype.descr + [(n,t) for n,t,_ in need])\n",
    "    for n in A.dtype.names:\n",
    "        B[n] = A[n]\n",
    "    for n,_,val in need:\n",
    "        B[n][:] = val\n",
    "    return B\n",
    "\n",
    "def _add_gp_value_column(A):\n",
    "    if \"GP_value_eV\" in (A.dtype.names or []):\n",
    "        return A\n",
    "    if \"E_gp_pred_eV\" not in A.dtype.names or \"E_prior_eV\" not in A.dtype.names:\n",
    "        return A\n",
    "    gp_value = A[\"E_gp_pred_eV\"].astype(float) + A[\"E_prior_eV\"].astype(float)\n",
    "    B = np.zeros(A.shape, dtype=A.dtype.descr + [(\"GP_value_eV\", float)])\n",
    "    for n in A.dtype.names:\n",
    "        B[n] = A[n]\n",
    "    B[\"GP_value_eV\"] = gp_value\n",
    "    return B\n",
    "\n",
    "def _dedup_by_step(A, keep=\"last\"):\n",
    "    step_f = A[\"Step\"].astype(float)\n",
    "    step_ix = np.rint(step_f).astype(np.int64)\n",
    "    off = np.abs(step_f - step_ix)\n",
    "    if np.any(off > STEP_EPS):\n",
    "        print(f\"[WARN] non-integer Step values (max off={off.max():.3g}); rounding applied.\")\n",
    "    index_by = {}\n",
    "    if keep == \"last\":\n",
    "        for i, s in enumerate(step_ix):\n",
    "            index_by[s] = i\n",
    "    else:\n",
    "        for i, s in enumerate(step_ix):\n",
    "            if s not in index_by:\n",
    "                index_by[s] = i\n",
    "    keep_idx = np.array(sorted(index_by.values(), key=lambda i: (step_ix[i], i)), dtype=int)\n",
    "    A2 = A[keep_idx]\n",
    "    order = np.argsort(np.rint(A2[\"Step\"]).astype(int))\n",
    "    A2 = A2[order]\n",
    "    keys = np.rint(A2[\"Step\"]).astype(np.int64)\n",
    "    if np.unique(keys).size != keys.size:\n",
    "        raise SystemExit(\"[ERR] de-dup by Step failed.\")\n",
    "    return A2, (len(A) - len(A2))\n",
    "\n",
    "def _drop_consecutive_value_dups(A, atol=VAL_ATOL):\n",
    "    Egp = A[\"E_gp_pred_eV\"].astype(float)\n",
    "    Var = A[\"Var_E_gp_pred_eV2\"].astype(float)\n",
    "    Epri = A[\"E_prior_eV\"].astype(float)\n",
    "    keep = [0]\n",
    "    for i in range(1, len(A)):\n",
    "        same = (\n",
    "            np.isclose(Egp[i], Egp[i-1], atol=atol) and\n",
    "            np.isclose(Var[i], Var[i-1], atol=atol) and\n",
    "            ((np.isnan(Epri[i]) and np.isnan(Epri[i-1])) or np.isclose(Epri[i], Epri[i-1], atol=atol))\n",
    "        )\n",
    "        if not same:\n",
    "            keep.append(i)\n",
    "    keep = np.array(keep, dtype=int)\n",
    "    return A[keep], (len(A) - len(keep))\n",
    "\n",
    "def _reindex_step(A):\n",
    "    B = A.copy()\n",
    "    B[\"Step\"] = np.arange(len(B), dtype=float)\n",
    "    return B\n",
    "\n",
    "def _save_structured(path: Path, A):\n",
    "    header = \",\".join(A.dtype.names)\n",
    "    np.savetxt(path, A, delimiter=\",\", header=header, comments=\"\", fmt=\"%.10g\")\n",
    "\n",
    "gp_clean_csv = None\n",
    "src = run_dir / CSV_IN\n",
    "if src.exists():\n",
    "    A = _load_structured(src)\n",
    "    A = _relabel(A)\n",
    "    A = _ensure_cols(A)\n",
    "    A = _add_gp_value_column(A)\n",
    "    n0 = len(A)\n",
    "    uniq0 = np.unique(np.rint(A[\"Step\"]).astype(np.int64)).size\n",
    "    if uniq0 < n0:\n",
    "        print(f\"[INFO] {run_dir.name}: {n0-uniq0} duplicate Step row(s) detected.\")\n",
    "    A1, rem1 = _dedup_by_step(A, keep=\"last\")\n",
    "    if rem1:\n",
    "        print(f\"[DE-DUP step] {run_dir.name}: removed {rem1} rows (kept last).\")\n",
    "    A2, rem2 = _drop_consecutive_value_dups(A1, atol=VAL_ATOL)\n",
    "    if rem2:\n",
    "        print(f\"[DE-DUP vals]  {run_dir.name}: removed {rem2} consecutive duplicates.\")\n",
    "    A3 = _reindex_step(A2)\n",
    "    A3 = _add_gp_value_column(A3)\n",
    "    out_path = run_dir / CSV_OUT\n",
    "    _save_structured(out_path, A3)\n",
    "    gp_clean_csv = out_path\n",
    "    print(f\"[CLEANED] {run_dir.name}: {n0} -> {len(A3)} rows written to {out_path.name}\")\n",
    "else:\n",
    "    print(f\"[GP] WARNING: {CSV_IN} missing in {run_dir}\")\n",
    "\n",
    "gp_df = None\n",
    "if gp_clean_csv and gp_clean_csv.exists():\n",
    "    A = _load_structured(gp_clean_csv)\n",
    "    gp_df = pd.DataFrame({name: A[name] for name in A.dtype.names})\n",
    "\n",
    "# GP length scale\n",
    "rx_hp_A = re.compile(r\"\\[HP\\]\\s*Training points:\\s*(\\d+),\\s*GP length scale:\\s*([+\\-]?\\d+(?:\\.\\d+)?(?:[eE][+\\-]?\\d+)?)\")\n",
    "rx_hp_B = re.compile(r\"\\[HP\\]\\s*Train\\s*pts:\\s*(\\d+)\\s*\\|\\s*scale=\\s*([+\\-]?\\d+(?:\\.\\d+)?(?:[eE][+\\-]?\\d+)?)\")\n",
    "rx_hp_C = re.compile(r\"scale\\s*=\\s*([+\\-]?\\d+(?:\\.\\d+)?(?:[eE][+\\-]?\\d+)?)\")\n",
    "\n",
    "hp_rows = []\n",
    "for d in run_dirs:\n",
    "    rid = d.name.split(\"_\")[-1]\n",
    "    try:\n",
    "        rid = int(rid)\n",
    "    except Exception:\n",
    "        pass\n",
    "    lp = d / LOGNAME\n",
    "    for line in lp.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines():\n",
    "        m = rx_hp_A.search(line) or rx_hp_B.search(line)\n",
    "        if m:\n",
    "            hp_rows.append({\"run\": rid, \"iter\": int(m.group(1)), \"scale\": float(m.group(2))})\n",
    "        else:\n",
    "            m2 = rx_hp_C.search(line)\n",
    "            if m2:\n",
    "                hp_rows.append({\n",
    "                    \"run\": rid,\n",
    "                    \"iter\": len([r for r in hp_rows if r[\"run\"] == rid]) + 1,\n",
    "                    \"scale\": float(m2.group(1))\n",
    "                })\n",
    "\n",
    "ls_df = None\n",
    "if hp_rows:\n",
    "    hp_long = pd.DataFrame(hp_rows).sort_values([\"run\", \"iter\"])\n",
    "    ls_df = hp_long[hp_long[\"run\"].astype(str) == str(median_run_id)][[\"iter\", \"scale\"]].rename(columns={\"iter\": \"Step\"})\n",
    "else:\n",
    "    print(\"[HP] WARNING: no [HP] lines matched; Panel B will display a notice.\")\n",
    "\n",
    "# Fingerprint data\n",
    "fp_df = None\n",
    "fp_csv = run_dir / \"fp_per_run.csv\"\n",
    "if fp_csv.exists():\n",
    "    fp_df = pd.read_csv(fp_csv)\n",
    "else:\n",
    "    print(f\"[FP] WARNING: {fp_csv.name} missing; Panel C may show a notice.\")\n",
    "\n",
    "# Plotting\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 200,\n",
    "    \"savefig.dpi\": 600,\n",
    "    \"pdf.fonttype\": 42,\n",
    "    \"ps.fonttype\": 42,\n",
    "    \"svg.fonttype\": \"none\",\n",
    "    \"font.size\": 18,\n",
    "    \"axes.titlesize\": 22,\n",
    "    \"axes.labelsize\": 20,\n",
    "    \"xtick.labelsize\": 18,\n",
    "    \"ytick.labelsize\": 18,\n",
    "    \"legend.fontsize\": 18,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False,\n",
    "    \"axes.linewidth\": 1.8,\n",
    "    \"lines.linewidth\": 2.6,\n",
    "})\n",
    "\n",
    "COLOR_GP, COLOR_PRIOR = \"tab:blue\", \"tab:green\"\n",
    "fig, axs = plt.subplots(3, 1, figsize=(16, 18), constrained_layout=True)\n",
    "fig.set_constrained_layout_pads(w_pad=0.3, wspace=0.3, h_pad=0.35, hspace=0.35)\n",
    "\n",
    "def set_beacon_axis(ax):\n",
    "    ax.set_xlim(0, 100)\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(10))\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(5))\n",
    "    ax.set_xlabel(\"BEACON step\")\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", length=9, width=1.8)\n",
    "    ax.tick_params(axis=\"both\", which=\"minor\", length=5, width=1.4)\n",
    "    ax.grid(True, alpha=0.25)\n",
    "\n",
    "# A) GP value + Prior (meV)\n",
    "axA = axs[0]\n",
    "if gp_df is not None and {\"Step\"}.issubset(gp_df.columns):\n",
    "    axA2 = axA.twinx()\n",
    "    step = np.rint(gp_df[\"Step\"].values).astype(int)\n",
    "    if \"GP_value_eV\" in gp_df.columns:\n",
    "        Emod = gp_df[\"GP_value_eV\"].astype(float).values\n",
    "    else:\n",
    "        Emod = gp_df[\"E_gp_pred_eV\"].astype(float).values + (\n",
    "            gp_df[\"E_prior_eV\"].astype(float).values if \"E_prior_eV\" in gp_df.columns else 0.0\n",
    "        )\n",
    "    Var = gp_df[\"Var_E_gp_pred_eV2\"].astype(float).values if \"Var_E_gp_pred_eV2\" in gp_df.columns else np.full_like(Emod, np.nan)\n",
    "    Epri = gp_df[\"E_prior_eV\"].astype(float).values if \"E_prior_eV\" in gp_df.columns else np.full_like(Emod, np.nan)\n",
    "    sigma = np.sqrt(np.clip(Var, 0.0, None))\n",
    "    axA.plot(step, Emod, \"-\", color=COLOR_GP, label=\"GP value\")\n",
    "    if np.isfinite(sigma).any():\n",
    "        axA.fill_between(step, Emod - sigma, Emod + sigma, alpha=0.25, color=COLOR_GP, label=\"GP ±1σ\")\n",
    "    if np.isfinite(Epri).any():\n",
    "        axA2.plot(step, 1000.0 * Epri, \"--\", color=COLOR_PRIOR, label=\"Prior (meV)\")\n",
    "    axA.set_ylabel(\"Energy (eV)\", color=COLOR_GP)\n",
    "    axA.tick_params(axis='y', labelcolor=COLOR_GP)\n",
    "    axA2.set_ylabel(\"Prior (meV)\", color=COLOR_PRIOR)\n",
    "    axA2.tick_params(axis='y', labelcolor=COLOR_PRIOR)\n",
    "    axA.set_title(\"A  GP value and prior vs optimisation step (median run)\")\n",
    "    set_beacon_axis(axA)\n",
    "    lines, labels = [], []\n",
    "    for ax in (axA, axA2):\n",
    "        h, lab = ax.get_legend_handles_labels()\n",
    "        lines.extend(h); labels.extend(lab)\n",
    "    if lines:\n",
    "        leg = axA.legend(lines, labels, loc=\"upper right\", framealpha=0.92, frameon=True)\n",
    "        leg.get_frame().set_linewidth(1.4)\n",
    "else:\n",
    "    axA.text(0.5, 0.5, \"GP CSV not found\", ha=\"center\", va=\"center\", transform=axA.transAxes)\n",
    "    axA.set_title(\"A  GP value and prior vs optimisation step (median run)\")\n",
    "    set_beacon_axis(axA)\n",
    "\n",
    "# B) GP length scale\n",
    "axB = axs[1]\n",
    "if ls_df is not None and not ls_df.empty:\n",
    "    axB.plot(ls_df[\"Step\"], ls_df[\"scale\"])\n",
    "    axB.set_ylabel(\"GP length scale\")\n",
    "else:\n",
    "    axB.text(0.5, 0.5, \"No [HP] length-scale data parsed\", ha=\"center\", va=\"center\", transform=axB.transAxes)\n",
    "axB.set_title(\"B  GP length scale (median run)\")\n",
    "set_beacon_axis(axB)\n",
    "\n",
    "# C) FP distance vs step (median run)\n",
    "axC = axs[2]\n",
    "plotted_any = False\n",
    "\n",
    "def _find_minima_csv():\n",
    "    for p in [PRIOR_DIR/\"minima_fps.csv\", Path.cwd()/\"minima_fps.csv\", Path(\"/mnt/data/minima_fps.csv\")]:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "if fp_df is not None:\n",
    "    fp_cols = [c for c in fp_df.columns if c.startswith(\"FP_\")]\n",
    "    step_col = None\n",
    "    for cand in (\"Step\", \"step\", \"STEP\"):\n",
    "        if cand in fp_df.columns:\n",
    "            step_col = cand\n",
    "            break\n",
    "    if fp_cols and step_col is not None:\n",
    "        steps  = np.rint(fp_df[step_col].to_numpy()).astype(int)\n",
    "        FP_run = fp_df[fp_cols].to_numpy(dtype=float)\n",
    "\n",
    "        mcsv = _find_minima_csv()\n",
    "        if mcsv is not None:\n",
    "            mins = pd.read_csv(mcsv)\n",
    "            mcols = [c for c in mins.columns if c.startswith(\"FP_\")]\n",
    "            if mcols and FP_run.shape[1] == mins[mcols].shape[1]:\n",
    "                M = mins[mcols].to_numpy(dtype=float)\n",
    "                names = mins[\"min_name\"].astype(str).tolist()\n",
    "\n",
    "                D_list = [np.linalg.norm(FP_run - M[j][None, :], axis=1) for j in range(len(names))]\n",
    "                D = np.vstack(D_list)\n",
    "                d_nearest = D.min(axis=0)\n",
    "\n",
    "                for j, nm in enumerate(names):\n",
    "                    axC.plot(steps, D[j], alpha=0.9, label=f\"Distance to {nm}\")\n",
    "                axC.plot(steps, d_nearest, lw=3.0, ls=\"--\", color=\"crimson\", alpha=0.95,\n",
    "                         label=\"Closest to any minimum\")\n",
    "                plotted_any = True\n",
    "\n",
    "if not plotted_any:\n",
    "    axC.text(0.5, 0.5, \"No minima refs or FP distances available\", ha=\"center\", va=\"center\", transform=axC.transAxes)\n",
    "\n",
    "axC.set_ylabel(\"FP distance (L2)\")\n",
    "axC.set_title(\"C  FP distance vs step (median run)\")\n",
    "set_beacon_axis(axC)\n",
    "\n",
    "# Legend for Panel C\n",
    "legC = axC.legend(loc=\"center left\", bbox_to_anchor=(1.02, 0.5), framealpha=0.92, frameon=True)\n",
    "legC.get_frame().set_linewidth(1.4)\n",
    "\n",
    "# Save\n",
    "fig.suptitle(label, y=0.995, fontsize=24)\n",
    "out_base = f\"{mol}_{prior_tag}_median_three_panel_gp_fp\"\n",
    "fig.savefig(PRIOR_DIR / f\"{out_base}.pdf\", bbox_inches=\"tight\", facecolor=\"white\")\n",
    "fig.savefig(PRIOR_DIR / f\"{out_base}.png\", dpi=600, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"[DONE] median run: {median_run_id}\\nSaved:\\n  {PRIOR_DIR / (out_base + '.pdf')}\\n  {PRIOR_DIR / (out_base + '.png')}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ase_env)",
   "language": "python",
   "name": "ase_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
